# GPT-ME
(A Repository containing the code I used to fine-tune a GPT-2 language model on my speech data.)

This summer, I trained OpenAI’s GPT-2 language model on personal database of my spoken words. Fed with language scraped from over 65 hours of audio—the data of my waking life—the model is an imprint of the patterns and probabilities guiding my verbal communication. Never before has a dataset like this been used to train a deep learning generative model, and though I have been working with the smallest variant of GPT-2 with only 117M parameters, my preliminary results are thought provoking—to say the least. 

To the uninitiated, GPT-2 is the second in a series of language models released by OpenAI. Trained with over 1.5 billion parameters on a massive corpus of text, the transformer-based model applies itself to a single task: to synthesize the next token in an arbitrary sequence. Leveraging the power of attention layers to model long term dependencies in language, the output of this probabilistic model is often so convincing that small-sized outputs are sometimes indifferentiable from human writing. Even more impressive than GPT-2 is its beefed-up cousin, GPT-3. Trained with over 175 billion parameters on the Common Crawl dataset, it is ten times larger than any other language model ever created. Though it is both eloquent and rather beautiful to behold, perhaps the most fascinating ability demonstrated by GPT-3—and the inspiration for this project—is its ability to develop a nuanced representation of “hidden” structures appearing in our language (a capability also shared by the GPT-2 model, though to a much smaller extent). For instance, across the corpus of text data is has been trained on, there are many examples of mathematical equations. It is from these equations that the model has internalized a rough idea of what math “is” and how it functions, a base of knowledge it can then apply to questions it has never seen before. In order to predict the next token in a sentence like “254 + 6508 = {next token}”, for example, the neural net utilizes an encoded probabilistic understanding of the patterns governing the context of sentences using “+” and “=” in order to give a correct answer (in this case, 6762). Though the model is trained exclusively on textual data, its capability in some respects goes further than the language itself, into that which language signifies: the entirety of our external world.

Hearing the exploits of the generalized intelligence of OpenAI’s GPT-3 model, I was motivated to examine linguistic modelling from a more personal perspective. If a language model can form a probabilistic understanding of “hidden” patterns in language, could it model that elusive idea of “self” that philosophers and psychologists alike have been attempting to model since the very beginning? 

To test my hypotheses, I needed to find a new source of data. Examining the GPT language models—though they are widely regarded as the most personal of the bunch—I still got the sense that neural nets do not mimetically represent the natural and tonal fluctuations of human communication. This is because the language they have been trained on is “written” rather than spoken. Does any of “ME” exist behind this text on this page? Has it not been edited, composed, and wrangled into that which is digestible to another eye? Constrained by style, performance, and the linear progression of meaning (think a Wikipedia article), written text will always be one step removed—the fossilized amber of a once fluid sap. Striving to strike at the very heart of my linguistic existence, I began to look towards more… organic… options. Like interleaved tangents extending out from the gravitational pull of a black hole, our spoken language shoots out from within, structuring the context that informs our assumptions about the conscious mind. Taking aim at the black box of consciousness, could the power of probability driving machine learning encode the patterns of my speech and then decode patterns of the mind? Despite the unvalidatable nature of the assumptions about consciousness grounding my research, one thing was certain: language was my way in. Like a spigot hammered into the tree of life, I taped the source.
