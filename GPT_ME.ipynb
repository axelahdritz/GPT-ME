{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPT-ME.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "cf61d8766260451d9ed06d97665d24a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_cab58fe9ca754593be5c151be3d17fd6",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_531a8103299b459f9bcb248b40668d2c",
              "IPY_MODEL_d13dd47b92f8432b99b1709db955af0f",
              "IPY_MODEL_438f95bfe0ed4ecea1333f7c4a4df7e4"
            ]
          }
        },
        "cab58fe9ca754593be5c151be3d17fd6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "531a8103299b459f9bcb248b40668d2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_fbf84b6318104bbf8e9965f6330626f9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c5bca91ff541426dbd6b365586b2ecaa"
          }
        },
        "d13dd47b92f8432b99b1709db955af0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f898bd1141434db29e9b86ff18962e03",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 548118077,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 548118077,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e8e41ac812ba4876b9faf36ca132fe14"
          }
        },
        "438f95bfe0ed4ecea1333f7c4a4df7e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f64c7a775c754d02abacf6991795a43d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 548M/548M [00:15&lt;00:00, 37.0MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_db0fbaaf6248484d96d5549a08fd6a51"
          }
        },
        "fbf84b6318104bbf8e9965f6330626f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c5bca91ff541426dbd6b365586b2ecaa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f898bd1141434db29e9b86ff18962e03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e8e41ac812ba4876b9faf36ca132fe14": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f64c7a775c754d02abacf6991795a43d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "db0fbaaf6248484d96d5549a08fd6a51": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/axelahdritz/GPT-ME/blob/main/GPT_ME.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKOTlwcmxmej"
      },
      "source": [
        "# GPT-ME:\n",
        "### *personalized language models and their psychological implications*\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKGBoVwuhM4H"
      },
      "source": [
        "This interactive python script was the backbone of my GPT-ME project. I am entirely indebted to Rey Farhan's illuminating [tutorial](https://reyfarhan.com/posts/easy-gpt2-finetuning-huggingface/), from which I constructed much of this fine-tuning script. For those that are interested, this is an application of the GPT-2 model found within the Huggingface library, and uses Pytorch to implement the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xf3Qw77SZGbS"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NmMdkZO8R6q"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCCeyhuDHdOu"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import datetime\n",
        "from google.colab import drive\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\n",
        "torch.manual_seed(42)\n",
        "\n",
        "from transformers import GPT2LMHeadModel,  GPT2Tokenizer, GPT2Config, GPT2LMHeadModel\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "satxtOn9CzgR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69da17c0-6393-4366-aabc-7d8cb3bf78ef"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Aug 22 16:36:04 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.57.02    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   46C    P0    31W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfdCML6Parvv"
      },
      "source": [
        "# Create Training Set\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EYFrNxr-TYb"
      },
      "source": [
        "# mount my Google Drive directory and access the training data located there\n",
        "gdrive_dir = '/content/gdrive/'\n",
        "drive.mount(gdrive_dir)\n",
        "\n",
        "filename = '/content/gdrive/MyDrive/Corpus/sentences.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ya3zsH0r-3JK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e788cf6-b49b-4ffd-bc0d-95b6beb7061d"
      },
      "source": [
        "# load into a data frame\n",
        "df = pd.read_csv(filename)  \n",
        "print(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                sentence  ...   original_audio\n",
            "0      I know. But fuck that. Yeah on see what I, wha...  ...  210717_2152.wav\n",
            "1       I have no clue what how things are going to b...  ...  210717_2152.wav\n",
            "2       Yes. Yes, animal thing that I have a couple g...  ...  210717_2152.wav\n",
            "3                                     I quit everything.  ...  210717_2152.wav\n",
            "4       I'd be lying if I said that I really knew how...  ...  210717_2152.wav\n",
            "...                                                  ...  ...              ...\n",
            "15398                   Take it. Take it. Take it, baby.  ...  210819_1629.wav\n",
            "15399                 Because I was a little short cuts.  ...  210819_1629.wav\n",
            "15400      Come on the phone. Give me my thoughts. Drop.  ...  210819_1629.wav\n",
            "15401                     Can I see what's the buttocks?  ...  210819_1629.wav\n",
            "15402                      I just got off. I can't find.  ...  210819_1629.wav\n",
            "\n",
            "[15403 rows x 6 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_U3m6wr3Ahzt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12985e7c-69a5-461a-84fb-e13ce8a94007"
      },
      "source": [
        "df.dropna(inplace=True) # remove NA values\n",
        "sentences = df.sentence.copy() # use the sentences only\n",
        "sentences"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        I know. But fuck that. Yeah on see what I, wha...\n",
              "1         I have no clue what how things are going to b...\n",
              "2         Yes. Yes, animal thing that I have a couple g...\n",
              "3                                       I quit everything.\n",
              "4         I'd be lying if I said that I really knew how...\n",
              "                               ...                        \n",
              "15398                     Take it. Take it. Take it, baby.\n",
              "15399                   Because I was a little short cuts.\n",
              "15400        Come on the phone. Give me my thoughts. Drop.\n",
              "15401                       Can I see what's the buttocks?\n",
              "15402                        I just got off. I can't find.\n",
              "Name: sentence, Length: 15403, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQ1oK0kXaV5p"
      },
      "source": [
        "We need to get an idea of how long our training documents are.\n",
        "\n",
        "I'm not going to use the same tokenizer as the GPT2 one, which is a [byte pair encoding tokenizer](https://blog.floydhub.com/tokenization-nlp/). Instead, I'm using a simple one just to get a rough understanding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKsH2sU0OCQA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "8ff69382-e86f-4c5d-f2d2-e1f839b2f4c1"
      },
      "source": [
        "doc_lengths = []\n",
        "\n",
        "for sentence in sentences:\n",
        "\n",
        "    # get rough token count distribution\n",
        "    tokens = nltk.word_tokenize(sentence)\n",
        "\n",
        "    doc_lengths.append(len(tokens))\n",
        "\n",
        "doc_lengths = np.array(doc_lengths)\n",
        "\n",
        "sns.distplot(doc_lengths)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f2551ed9950>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63t_69HjlwAj"
      },
      "source": [
        "np.average(doc_lengths)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tuq5bqdr4_a6"
      },
      "source": [
        "Even though these token counts won't match up to the BPE tokenizer's, I'm confident that most responses will fit under the 768 embedding size limit for the small GPT2 model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMml12FJGjPW"
      },
      "source": [
        "# GPT2 Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANJhbBwdxN-b"
      },
      "source": [
        "Although the defaults take care of this,I thought I'd show that you can specify some of the special tokens. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z474sSC6oe7A"
      },
      "source": [
        "# Load the GPT tokenizer.\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>') #gpt2-medium\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sh0XKuDvnryn"
      },
      "source": [
        "print(\"The max model length is {} for this model, although the actual embedding size for GPT small is 768\".format(tokenizer.model_max_length))\n",
        "print(\"The beginning of sequence token {} token has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.bos_token_id), tokenizer.bos_token_id))\n",
        "print(\"The end of sequence token {} has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.eos_token_id), tokenizer.eos_token_id))\n",
        "print(\"The padding token {} has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.pad_token_id), tokenizer.pad_token_id))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ex5O1eV-Pfct"
      },
      "source": [
        "# PyTorch Datasets & Dataloaders\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lgZoOYkxZfx"
      },
      "source": [
        "GPT2 is a large model. Increasing the batch size above 2 has lead to out of memory problems. This can be mitigated by accumulating the gradients but that is out of scope here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scqrzmqhV__z"
      },
      "source": [
        "batch_size = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqGMee7Isfpx"
      },
      "source": [
        "I'm using the standard PyTorch approach of loading data in using a [dataset class](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html).\n",
        "\n",
        "I'm passing in the tokenizer as an argument but normally I would  instantiate it within the class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_XJVIetKN-h"
      },
      "source": [
        "class GPT2Dataset(Dataset):\n",
        "\n",
        "  def __init__(self, txt_list, tokenizer, gpt2_type=\"gpt2\", max_length=300):\n",
        "\n",
        "    self.tokenizer = tokenizer\n",
        "    self.input_ids = []\n",
        "    self.attn_masks = []\n",
        "\n",
        "    for txt in txt_list:\n",
        "\n",
        "      encodings_dict = tokenizer('<|startoftext|>'+ txt + '<|endoftext|>', truncation=True, max_length=max_length, padding=\"max_length\")\n",
        "\n",
        "      self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
        "      self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
        "    \n",
        "  def __len__(self):\n",
        "    return len(self.input_ids)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.input_ids[idx], self.attn_masks[idx] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89Z7aYUgpWrd"
      },
      "source": [
        "To understand how I've used the tokenizer, it's worth reading [the docs](https://huggingface.co/transformers/main_classes/tokenizer.html). I've wrapped each bio in the bos and eos tokens.\n",
        "\n",
        "Every tensor passed to the model should be the same length.\n",
        "\n",
        "If the bio is shorter than 768 tokens, it will be padded to a length of 768 using the padding token. In addition, an attention mask will be returned that needs to be passed to the model to tell it to ignore the padding tokens. \n",
        "\n",
        "If the data is longer than 768 tokens, it will be truncated without the eos_token. This isn't a problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xza_O1_rD7yh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53402878-1c3d-4c80-ce7b-4d8bb54bcd4e"
      },
      "source": [
        "dataset = GPT2Dataset(sentences, tokenizer, max_length=300)\n",
        "\n",
        "# Split into training and validation sets\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "print('{:>5,} training samples'.format(train_size))\n",
        "print('{:>5,} validation samples'.format(val_size))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13,862 training samples\n",
            "1,541 validation samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0WeP5PREUuy"
      },
      "source": [
        "# Create the DataLoaders for our training and validation datasets.\n",
        "# We'll take training samples in random order. \n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  # The training samples.\n",
        "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6TKgyUzPIQc"
      },
      "source": [
        "# Finetune GPT2 Language Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFsCTp_mporB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "cf61d8766260451d9ed06d97665d24a9",
            "cab58fe9ca754593be5c151be3d17fd6",
            "531a8103299b459f9bcb248b40668d2c",
            "d13dd47b92f8432b99b1709db955af0f",
            "438f95bfe0ed4ecea1333f7c4a4df7e4",
            "fbf84b6318104bbf8e9965f6330626f9",
            "c5bca91ff541426dbd6b365586b2ecaa",
            "f898bd1141434db29e9b86ff18962e03",
            "e8e41ac812ba4876b9faf36ca132fe14",
            "f64c7a775c754d02abacf6991795a43d",
            "db0fbaaf6248484d96d5549a08fd6a51"
          ]
        },
        "outputId": "4e730dd0-7a64-470a-e802-325fc69885df"
      },
      "source": [
        "configuration = GPT2Config.from_pretrained('gpt2', output_hidden_states=False)\n",
        "\n",
        "# instantiate the model\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", config=configuration)\n",
        "\n",
        "# this step is necessary because I've added some tokens (bos_token, etc) to the embeddings\n",
        "# otherwise the tokenizer and model tensors won't match up\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "device = torch.device(\"cuda\")\n",
        "model.cuda()\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cf61d8766260451d9ed06d97665d24a9",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBEVY2PYSTXJ"
      },
      "source": [
        "# some parameters I cooked up that work reasonably well\n",
        "\n",
        "epochs = 3\n",
        "learning_rate = 5e-4\n",
        "warmup_steps = 1e2\n",
        "epsilon = 1e-8\n",
        "\n",
        "# this produces sample output every 100 steps\n",
        "sample_every = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLs72DuMODJO"
      },
      "source": [
        "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = learning_rate,\n",
        "                  eps = epsilon\n",
        "                )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-p0upAhhRiIx"
      },
      "source": [
        "# Total number of training steps is [number of batches] x [number of epochs]. \n",
        "# (Note that this is not the same as the number of training samples).\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "# This changes the learning rate as the training loop progresses\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = warmup_steps, \n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpt6tR83keZD"
      },
      "source": [
        "def format_time(elapsed):\n",
        "    return str(datetime.timedelta(seconds=int(round((elapsed)))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCPohrZ-CTWu"
      },
      "source": [
        "total_t0 = time.time()\n",
        "\n",
        "training_stats = []\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "for epoch_i in range(0, epochs):\n",
        "\n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    total_train_loss = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_labels = batch[0].to(device)\n",
        "        b_masks = batch[1].to(device)\n",
        "\n",
        "        model.zero_grad()        \n",
        "\n",
        "        outputs = model(  b_input_ids,\n",
        "                          labels=b_labels, \n",
        "                          attention_mask = b_masks,\n",
        "                          token_type_ids=None\n",
        "                        )\n",
        "\n",
        "        loss = outputs[0]  \n",
        "\n",
        "        batch_loss = loss.item()\n",
        "        total_train_loss += batch_loss\n",
        "\n",
        "        # Get sample every x batches.\n",
        "        if step % sample_every == 0 and not step == 0:\n",
        "\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            print('  Batch {:>5,}  of  {:>5,}. Loss: {:>5,}.   Elapsed: {:}.'.format(step, len(train_dataloader), batch_loss, elapsed))\n",
        "\n",
        "            model.eval()\n",
        "\n",
        "            sample_outputs = model.generate(\n",
        "                                    bos_token_id=random.randint(1,30000),\n",
        "                                    do_sample=True,   \n",
        "                                    top_k=50, \n",
        "                                    max_length = 200,\n",
        "                                    top_p=0.95, \n",
        "                                    num_return_sequences=1\n",
        "                                )\n",
        "            for i, sample_output in enumerate(sample_outputs):\n",
        "                  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))\n",
        "            \n",
        "            model.train()\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)       \n",
        "    \n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epoch took: {:}\".format(training_time))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_labels = batch[0].to(device)\n",
        "        b_masks = batch[1].to(device)\n",
        "        \n",
        "        with torch.no_grad():        \n",
        "\n",
        "            outputs  = model(b_input_ids, \n",
        "#                            token_type_ids=None, \n",
        "                             attention_mask = b_masks,\n",
        "                            labels=b_labels)\n",
        "          \n",
        "            loss = outputs[0]  \n",
        "            \n",
        "        batch_loss = loss.item()\n",
        "        total_eval_loss += batch_loss        \n",
        "\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "    \n",
        "    validation_time = format_time(time.time() - t0)    \n",
        "\n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQTvJ1vRP7u4"
      },
      "source": [
        "Let's view the summary of the training process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6O_NbXFGMukX"
      },
      "source": [
        "# Display floats with two decimal places.\n",
        "pd.set_option('precision', 2)\n",
        "\n",
        "# Create a DataFrame from our training statistics.\n",
        "df_stats = pd.DataFrame(data=training_stats)\n",
        "\n",
        "# Use the 'epoch' as the row index.\n",
        "df_stats = df_stats.set_index('epoch')\n",
        "\n",
        "# A hack to force the column headers to wrap.\n",
        "df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
        "\n",
        "# Display the table.\n",
        "df_stats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68xreA9JAmG5"
      },
      "source": [
        "# Use plot styling from seaborn.\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# Plot the learning curve.\n",
        "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
        "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
        "\n",
        "# Label the plot.\n",
        "plt.title(\"Training & Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.xticks([1, 2, 3, 4])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfjYoa6WmkN6"
      },
      "source": [
        "# Display Model Info"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PIiVlDYCtSq"
      },
      "source": [
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(model.named_parameters())\n",
        "\n",
        "print('The GPT-2 model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "\n",
        "for p in params[0:2]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[2:14]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-2:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2079Qyn8Mt8"
      },
      "source": [
        "# Saving & Loading Fine-Tuned Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ulTWaOr8QNY"
      },
      "source": [
        "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
        "\n",
        "output_dir = '/content/gdrive/MyDrive/model_save2/'\n",
        "\n",
        "# Create output directory if needed\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "# They can then be reloaded using `from_pretrained()`\n",
        "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "model_to_save.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqMzI3VTCZo5"
      },
      "source": [
        "!ls -l --block-size=K /content/gdrive/MyDrive/model_save1/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WUFUIQ8Cu8D"
      },
      "source": [
        "!ls -l --block-size=M /content/gdrive/MyDrive/model_save1/pytorch_model.bin"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxlZsafTC-V5"
      },
      "source": [
        "#output_dir = '/content/gdrive/MyDrive/model_save1'\n",
        "# # Load a trained model and vocabulary that you have fine-tuned\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "#device = torch.device(\"cuda\")\n",
        "#model.cuda()\n",
        "#model = GPT2LMHeadModel.from_pretrained(output_dir)\n",
        "#tokenizer = GPT2Tokenizer.from_pretrained(output_dir)\n",
        "#model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLf6rbRglYhQ"
      },
      "source": [
        "# Generate Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4XhewaV93-_"
      },
      "source": [
        "model.eval()\n",
        "\n",
        "prompt = \"<|startoftext|> The world is too small for me.\"\n",
        "\n",
        "generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
        "generated = generated.to(device)\n",
        "\n",
        "print(generated)\n",
        "\n",
        "sample_outputs = model.generate(\n",
        "                                generated, \n",
        "                                #bos_token_id=random.randint(1,30000),\n",
        "                                do_sample=True,   \n",
        "                                top_k=50, \n",
        "                                min_length = 20,\n",
        "                                max_length = 900,\n",
        "                                top_p=0.95, \n",
        "                                num_return_sequences=10\n",
        "                                )\n",
        "\n",
        "for i, sample_output in enumerate(sample_outputs):\n",
        "  print(\"{}: {}\\n\\n\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}